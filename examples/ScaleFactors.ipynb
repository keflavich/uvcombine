{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for finding the scaling factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook compares different methods of finding the scaling factor to be applied to the single-dish data. uv-samples where the interometer-ed image and single-dish image overlap are compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ipython notebook setup:\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "pl.rcParams['image.interpolation'] = 'nearest'\n",
    "pl.rcParams['image.origin'] = 'lower'\n",
    "pl.rcParams['figure.figsize'] = (12,8)\n",
    "pl.rcParams['image.cmap'] = 'viridis'\n",
    "pl.rcParams['patch.edgecolor'] = 'none'\n",
    "pl.rcParams['axes.prop_cycle'] = pl.cycler('color', ('#338ADD', '#9A44B6', '#A60628', '#467821', '#CF4457', '#188487', '#E24A33'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import image_registration\n",
    "from astropy import convolution\n",
    "import numpy as np\n",
    "from uvcombine.uvcombine import feather_compare\n",
    "from uvcombine.scale_factor import find_effSDbeam\n",
    "# from uvcombine.tests.utils import generate_test_fits\n",
    "from astropy.utils.console import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an input image with specified parameters\n",
    "# (this can later be modified - it will be good to examine the effects of\n",
    "# different power laws, different types of input...)\n",
    "# We're assuming a scale of 1\"/pixel for this example\n",
    "np.random.seed(0)\n",
    "imsize = 1024\n",
    "im = image_registration.tests.make_extended(imsize=imsize, powerlaw=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ygrid, xgrid = np.indices(im.shape, dtype='float')\n",
    "rr = ((xgrid-im.shape[1]/2)**2+(ygrid-im.shape[0]/2)**2)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a UV sampling mask.\n",
    "# This removes all large-angular scale (r<8) features *in UV space* and all\n",
    "# small angular scales.\n",
    "# In fourier space, r=0 corresponds to the DC component\n",
    "# r=1 corresponds to the full map (one period over that map)\n",
    "# r=256 is the smallest angular scale, which is 2 pixels (nyquist sampling....?)\n",
    "# We're assuming a pixel scale of 1\" / pixel\n",
    "# therefore 56\" corresponds to 9m at 2mm (i.e., nearly the closest spacing possible for 7m)\n",
    "# We cut off the \"interferometer\" at 2.5\" resolution\n",
    "largest_scale = 56.\n",
    "smallest_scale = 2.5\n",
    "image_scale = im.shape[0] # assume symmetric (default=256)\n",
    "ring = (rr>=(image_scale/largest_scale)) & (rr<=(image_scale/smallest_scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the interferometric map by removing both large and small angular\n",
    "# scales in fourier space\n",
    "imfft = np.fft.fft2(im)\n",
    "imfft_interferometered = imfft * np.fft.fftshift(ring)\n",
    "im_interferometered = np.fft.ifft2(imfft_interferometered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the single-dish map by convolving the image with a FWHM=40\" kernel\n",
    "# (this interpretation is much easier than the sharp-edged stuff in fourier\n",
    "# space because the kernel is created in real space)\n",
    "lowresfwhm = 30\n",
    "singledish_im = convolution.convolve_fft(im,\n",
    "                                         convolution.Gaussian2DKernel(lowresfwhm/2.35),\n",
    "                                         boundary='fill', fill_value=im.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'u' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-56dfd9f908e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# make some headers for HDUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpixel_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marcsec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbeamfwhm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlowresfwhm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marcsec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrestfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspectral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m header = {'CDELT1': -(pixel_scale).to(u.deg).value,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'u' is not defined"
     ]
    }
   ],
   "source": [
    "# make some headers for HDUs\n",
    "pixel_scale = 1 * u.arcsec\n",
    "beamfwhm = lowresfwhm * u.arcsec\n",
    "restfreq = (2 * u.mm).to(u.Hz, u.spectral())\n",
    "header = {'CDELT1': -(pixel_scale).to(u.deg).value,\n",
    "              'CDELT2': (pixel_scale).to(u.deg).value,\n",
    "              'BMAJ': beamfwhm.to(u.deg).value,\n",
    "              'BMIN': beamfwhm.to(u.deg).value,\n",
    "              'BPA': 0.0,\n",
    "              'CRPIX1': imsize/2.,\n",
    "              'CRPIX2': imsize/2.,\n",
    "              'CRVAL1': 0.0,\n",
    "              'CRVAL2': 0.0,\n",
    "              'CTYPE1': 'GLON-CAR',\n",
    "              'CTYPE2': 'GLAT-CAR',\n",
    "              'CUNIT1': 'deg',\n",
    "              'CUNIT2': 'deg',\n",
    "              'CRVAL3': restfreq.to(u.Hz).value,\n",
    "              'CUNIT3': 'Hz',\n",
    "              'CDELT3': 1e6, # 1 MHz; doesn't matter\n",
    "              'CRPIX3': 1,\n",
    "              'CTYPE3': 'FREQ',\n",
    "              'RESTFRQ': restfreq.to(u.Hz).value,\n",
    "              'BUNIT': 'MJy/sr',\n",
    "             }\n",
    "header = fits.Header(header)\n",
    "\n",
    "interf_header = header.copy()\n",
    "interf_fwhm = smallest_scale * u.arcsec\n",
    "interf_header[\"BMAJ\"] = interf_fwhm.to(u.deg).value\n",
    "interf_header[\"BMIN\"] = interf_fwhm.to(u.deg).value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the Correct single-dish FWHM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feathering two datasets together depends critically on the single-dish beam model. The SD beam acts to weight the data at different scales and an incorrect weighting at certain scales will skew the feathered image. This is particularly important in the overlap region, which we use to determine if a scaling factor needs to be applied to one of the datasets. \n",
    "\n",
    "Here, we check the effects of assuming the wrong single-dish FWHM when calculating the scale factor. From Chapter 3 in [Stanimirovic (1999)](https://ui.adsabs.harvard.edu/#abs/1999PhDT........21S/abstract), the ratios will have a quadratic dependence of the wavenumber when the wrong FWHM is used:\n",
    "$$\n",
    "\\mathcal{f}_{\\rm cal} \\approx 1 + \\frac{\\Delta\\theta \\left( 2\\theta_0 + \\Delta\\theta \\right)}{4 {\\rm ln}2} k^2,\n",
    "$$\n",
    "where $k$ is the angular scale, $\\theta_0$ is the true beam size, and $\\Delta\\theta << \\theta_0$ is the difference between the actual and assumed beam size. The relation should then be quadratic: $\\mathcal{f}_{\\rm cal} \\sim M + N k^2.$ (Eqs. 3.10 \\& 3.11). The correct beam size should have $\\Delta\\theta = 0$ and so to check for the correct beam model, we can find where the slope $N$ is closest to zero.\n",
    "\n",
    "In the example below, the true SD beam size is 30''. To check this relation, we will try beams ranging from 18'' to 46''."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interferometer_hdu = fits.PrimaryHDU(im_interferometered.real, header=interf_header)\n",
    "singledish_hdu = fits.PrimaryHDU(singledish_im, header=header)\n",
    "\n",
    "# Sample a large range of FWHMs\n",
    "lowresfwhms = np.arange(18, 46, 0.25) * u.arcsec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `find_effSDbeam` takes the samples in the overlap region of the datasets and fits a linear relation between the ratios between the high- and low-resolution data against the angular scale squared. The plot below shows the slope $N$ as a function of the assumed SD beam size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slopes, slopes_CI = \\\n",
    "    find_effSDbeam(interferometer_hdu, singledish_hdu, largest_scale * u.arcsec, lowresfwhms,\n",
    "                   verbose=True)\n",
    "pl.axvline(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The behaviour is close to quadratic, as expected, and the zero-crossing point is near to the true SD beam size. As $\\Delta\\theta$ gets largers, the approximation breaks down. This happens when assuming a larger beam as the curve flattens out in this regime. This suggests that overestimating the SD beam will be equally as biased for a large range.\n",
    "\n",
    "The actual zero crossing point is slightly smaller than the beam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl.errorbar(lowresfwhms.to(u.arcsec).value, slopes,\n",
    "            yerr=[slopes - slopes_CI[0],\n",
    "                  slopes_CI[1] - slopes])\n",
    "\n",
    "pl.ylabel(\"Slope\")\n",
    "pl.xlabel(\"Low Res. FWHM (arcsec)\")\n",
    "pl.xlim([27, 33])\n",
    "pl.ylim([-100, 100])\n",
    "pl.axvline(30, linestyle='--', zorder=-1)\n",
    "pl.axhline(0, linestyle='--', zorder=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a better estimate of the root by fitting a spline to the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "\n",
    "yToFind = 0\n",
    "yreduced = slopes - yToFind\n",
    "freduced = interpolate.UnivariateSpline(lowresfwhms.value, yreduced, s=1e5)\n",
    "freduced.roots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl.errorbar(lowresfwhms.to(u.arcsec).value, slopes,\n",
    "            yerr=[slopes - slopes_CI[0],\n",
    "                  slopes_CI[1] - slopes])\n",
    "pl.plot(lowresfwhms.value, freduced(lowresfwhms.value))\n",
    "pl.xlim([27, 33])\n",
    "pl.ylim([-100, 100])\n",
    "pl.axvline(30, linestyle='--', zorder=-1)\n",
    "pl.axhline(0, linestyle='--', zorder=-1)\n",
    "# Closest spline root to the FWHM\n",
    "pl.axvline(freduced.roots()[0], linestyle=\"-\", color='r', zorder=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example underestimates the true beam size by about 1\", but does demonstrate that the method returns a good approximation to the true value. \n",
    "\n",
    "Is the beam always underestimated with this method? To test whether the method does this consistently, we will create 100 instances of power-law images with the same properties (**This might take awhile to run!**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_effSDbeam_find(niters=100, imsize=1024, powerlaw=1.5, lowresfwhm=30 * u.arcsec, interf_las=56 * u.arcsec,\n",
    "                        interf_sas=2.5 * u.arcsec, pixel_scale=1 * u.arcsec,\n",
    "                        test_range=np.arange(18, 46, 0.5) * u.arcsec,\n",
    "                        wavelength=2 * u.mm):\n",
    "    \n",
    "    found_sdbeam = np.empty((niters))\n",
    "    \n",
    "    for i in ProgressBar(niters):\n",
    "    \n",
    "        im = image_registration.tests.make_extended(imsize=imsize, powerlaw=powerlaw)\n",
    "        ygrid, xgrid = np.indices(im.shape, dtype='float')\n",
    "        rr = ((xgrid-im.shape[1]/2)**2+(ygrid-im.shape[0]/2)**2)**0.5\n",
    "        image_scale = im.shape[0] # assume symmetric (default=256)\n",
    "        ring = (rr>=(image_scale/largest_scale)) & (rr<=(image_scale/smallest_scale))\n",
    "    \n",
    "        imfft = np.fft.fft2(im)\n",
    "        imfft_interferometered = imfft * np.fft.fftshift(ring)\n",
    "        im_interferometered = np.fft.ifft2(imfft_interferometered)\n",
    "    \n",
    "        singledish_im = convolution.convolve_fft(im,\n",
    "                                                 convolution.Gaussian2DKernel(lowresfwhm.value / np.sqrt(8 * np.log(2))),\n",
    "                                                 boundary='fill', fill_value=im.mean())\n",
    "    \n",
    "        beamfwhm = lowresfwhm\n",
    "        restfreq = wavelength.to(u.Hz, u.spectral())\n",
    "        header = {'CDELT1': -(pixel_scale).to(u.deg).value,\n",
    "                  'CDELT2': (pixel_scale).to(u.deg).value,\n",
    "                  'BMAJ': beamfwhm.to(u.deg).value,\n",
    "                  'BMIN': beamfwhm.to(u.deg).value,\n",
    "                  'BPA': 0.0,\n",
    "                  'CRPIX1': imsize/2.,\n",
    "                  'CRPIX2': imsize/2.,\n",
    "                  'CRVAL1': 0.0,\n",
    "                  'CRVAL2': 0.0,\n",
    "                  'CTYPE1': 'GLON-CAR',\n",
    "                  'CTYPE2': 'GLAT-CAR',\n",
    "                  'CUNIT1': 'deg',\n",
    "                  'CUNIT2': 'deg',\n",
    "                  'CRVAL3': restfreq.value,\n",
    "                  'CUNIT3': 'Hz',\n",
    "                  'CDELT3': 1e6, # 1 MHz; doesn't matter\n",
    "                  'CRPIX3': 1,\n",
    "                  'CTYPE3': 'FREQ',\n",
    "                  'RESTFRQ': restfreq.to(u.Hz).value,\n",
    "                  'BUNIT': 'MJy/sr',\n",
    "                 }\n",
    "        header = fits.Header(header)\n",
    "\n",
    "        interf_header = header.copy()\n",
    "        interf_fwhm = interf_sas\n",
    "        interf_header[\"BMAJ\"] = interf_fwhm.to(u.deg).value\n",
    "        interf_header[\"BMIN\"] = interf_fwhm.to(u.deg).value\n",
    "    \n",
    "        interferometer_hdu = fits.PrimaryHDU(im_interferometered.real, header=interf_header)\n",
    "        singledish_hdu = fits.PrimaryHDU(singledish_im, header=header)\n",
    "    \n",
    "        slopes, slopes_CI = \\\n",
    "            find_effSDbeam(interferometer_hdu, singledish_hdu, interf_las, test_range,\n",
    "                           verbose=False)\n",
    "    \n",
    "        yToFind = 0\n",
    "        yreduced = slopes - yToFind\n",
    "        freduced = interpolate.UnivariateSpline(test_range.value, yreduced, s=1e5)\n",
    "        closest_fwhm = freduced.roots()[np.argmin(np.abs(freduced.roots() - lowresfwhm.value))]\n",
    "        \n",
    "        found_sdbeam[i] = closest_fwhm\n",
    "        \n",
    "    return found_sdbeam * test_range.unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mc_effSDbeams = test_effSDbeam_find(niters=100, test_range=np.arange(18, 46, 0.5) * u.arcsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = pl.hist(mc_effSDbeams.value, bins=15)\n",
    "pl.axvline(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! The point where the slope is zero tends to be underestimated by about 1''. But this does happen to be the size of a pixel in the test data, so choosing a finer spatial grid might help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining the scale factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the correct beam model, the bias from choosing an incorrect beam model is removed. Now we want to calculate the overall scale factor between the datasets.\n",
    "\n",
    "With a single plane, and limited uv-overlap, the number of samples to derive the scale factor is quite limited. If instead we have a spectral-line data cube, the samples may be combined to examine their distribution. This assumes that the noise is roughly constant in the different channels. If the data has a spectrally varying noise, a subset of the channels should be chosen.\n",
    "\n",
    "With simulated data, what does the distribution of the ratios in the overlap region look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nplanes = 100\n",
    "ratios = []\n",
    "highres_pts = []\n",
    "lowres_pts = []\n",
    "wavelength = 2 * u.mm\n",
    "\n",
    "for _ in ProgressBar(nplanes):\n",
    "        im = image_registration.tests.make_extended(imsize=imsize, powerlaw=1.5)\n",
    "        # Let's add a bit of noise\n",
    "        im += np.random.normal(0, 0.8, size=im.shape)\n",
    "        ygrid, xgrid = np.indices(im.shape, dtype='float')\n",
    "        rr = ((xgrid-im.shape[1]/2)**2+(ygrid-im.shape[0]/2)**2)**0.5\n",
    "        image_scale = im.shape[0] # assume symmetric (default=256)\n",
    "        ring = (rr>=(image_scale/largest_scale)) & (rr<=(image_scale/smallest_scale))\n",
    "    \n",
    "        imfft = np.fft.fft2(im)\n",
    "        imfft_interferometered = imfft * np.fft.fftshift(ring)\n",
    "        im_interferometered = np.fft.ifft2(imfft_interferometered)\n",
    "    \n",
    "        singledish_im = convolution.convolve_fft(im,\n",
    "                                                 convolution.Gaussian2DKernel(lowresfwhm / np.sqrt(8 * np.log(2))),\n",
    "                                                 boundary='fill', fill_value=im.mean())\n",
    "    \n",
    "        beamfwhm = lowresfwhm * u.arcsec\n",
    "        restfreq = wavelength.to(u.Hz, u.spectral())\n",
    "        header = {'CDELT1': -(pixel_scale).to(u.deg).value,\n",
    "                  'CDELT2': (pixel_scale).to(u.deg).value,\n",
    "                  'BMAJ': beamfwhm.to(u.deg).value,\n",
    "                  'BMIN': beamfwhm.to(u.deg).value,\n",
    "                  'BPA': 0.0,\n",
    "                  'CRPIX1': imsize/2.,\n",
    "                  'CRPIX2': imsize/2.,\n",
    "                  'CRVAL1': 0.0,\n",
    "                  'CRVAL2': 0.0,\n",
    "                  'CTYPE1': 'GLON-CAR',\n",
    "                  'CTYPE2': 'GLAT-CAR',\n",
    "                  'CUNIT1': 'deg',\n",
    "                  'CUNIT2': 'deg',\n",
    "                  'CRVAL3': restfreq.value,\n",
    "                  'CUNIT3': 'Hz',\n",
    "                  'CDELT3': 1e6, # 1 MHz; doesn't matter\n",
    "                  'CRPIX3': 1,\n",
    "                  'CTYPE3': 'FREQ',\n",
    "                  'RESTFRQ': restfreq.to(u.Hz).value,\n",
    "                  'BUNIT': 'MJy/sr',\n",
    "                 }\n",
    "        header = fits.Header(header)\n",
    "\n",
    "        interf_header = header.copy()\n",
    "        interf_fwhm = smallest_scale * u.arcsec\n",
    "        interf_header[\"BMAJ\"] = interf_fwhm.to(u.deg).value\n",
    "        interf_header[\"BMIN\"] = interf_fwhm.to(u.deg).value\n",
    "    \n",
    "        interferometer_hdu = fits.PrimaryHDU(im_interferometered.real, header=interf_header)\n",
    "        singledish_hdu = fits.PrimaryHDU(singledish_im, header=header)\n",
    "        \n",
    "        plane_samples = feather_compare(interferometer_hdu, singledish_hdu, SAS=lowresfwhm * u.arcsec,\n",
    "                                        LAS=largest_scale * u.arcsec, lowresfwhm=lowresfwhm * u.arcsec,\n",
    "                                        return_samples=True, doplot=False)\n",
    "        ratios.append(plane_samples[1])\n",
    "        highres_pts.append(plane_samples[2])\n",
    "        lowres_pts.append(plane_samples[3])\n",
    "        \n",
    "ratios = np.hstack(ratios)\n",
    "highres_pts = np.hstack(highres_pts)\n",
    "lowres_pts = np.hstack(lowres_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There's some sort of numerical effect going on in the interferometer image. Remove the low outliers at the\n",
    "# float precision level\n",
    "good_pts = np.where(np.log(highres_pts) > -20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's helpful to examine the distribution of the high-to-low ratios in log space. As we'll see below, noise will cause outliers at both small and large values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from astropy.visualization import hist\n",
    "_ = hist(np.log(ratios[good_pts]), bins='scott', normed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratios have strong tails in both direction. In log-space, the low-end tail is more prominent than the upper because of the image is defined as a power-law with an index $<2$. The following analysis treats these tails as symmetric, which is appropriate for images dominated by a log-normal distribution. This tends to be a better approximation for observational data.\n",
    "\n",
    "Why do we get these tails? The histograms of the intensity values from the low- and high-resolution data are (not quite) log-normals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = hist(np.log(highres_pts[good_pts]), bins='scott', normed=True, alpha=0.3, label=\"High-res\")\n",
    "_ = hist(np.log(lowres_pts), bins='scott', normed=True, alpha=0.3, label=\"Low-res\")\n",
    "p.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be more appropriate to treat each distribution as a power-law log-normal distribution. For the purposes of showing how to get the scale factor, assume these are log-normal variates (because it's a lot easier).\n",
    "\n",
    "The ratio of two normal random variates results in a Cauchy distribution. So the ratio of two log-normally distributed variables is a Cauchy distribution in log-space. Thus to find the scaling factor, the log of the ratio is fit to a Cauchy distribution. Its median is then an estimate of the scaling factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "cauchy_fit = stats.cauchy.fit(np.log(ratios[good_pts]))\n",
    "print(cauchy_fit)\n",
    "print(np.exp(np.array(cauchy_fit)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = hist(np.log(ratios[good_pts]), bins='scott', normed=True)\n",
    "pl.plot(np.arange(-4, 4, 0.1), stats.cauchy.pdf(np.arange(-4, 4, 0.1), *cauchy_fit))\n",
    "pl.axvline(np.log(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaling factor is below 1, as expected due to the heavier tail at low values.\n",
    "\n",
    "The scipy method gives no estimate of uncertainties. Adopting a likelihood model with statsmodels provides an estimate of the parameter uncertainties *assuming normality*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from statsmodels.base.model import GenericLikelihoodModel\n",
    "except ImportError:\n",
    "    raise ImportError(\"statsmodels must be installed to run this cell.\")\n",
    "\n",
    "class Likelihood(GenericLikelihoodModel):\n",
    "\n",
    "    # Get the number of parameters from shapes.\n",
    "    # Add one for scales, since we're assuming loc is frozen.\n",
    "    # Keeping loc=0 is appropriate for log-normal models.\n",
    "    nparams = 1 if stats.cauchy.shapes is None else \\\n",
    "        len(stats.cauchy.shapes.split(\",\")) + 1\n",
    "\n",
    "    def loglike(self, params):\n",
    "        if np.isnan(params).any():\n",
    "            return - np.inf\n",
    "\n",
    "        loglikes = \\\n",
    "            stats.cauchy.logpdf(self.endog, *params[:-2],\n",
    "                                scale=params[-1],\n",
    "                                loc=params[-2])\n",
    "        if not np.isfinite(loglikes).all():\n",
    "            return - np.inf\n",
    "        else:\n",
    "            return loglikes.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mle_model = Likelihood(np.log(ratios))\n",
    "fitted_model = mle_model.fit(cauchy_fit, method='nm')\n",
    "fitted_model.df_model = len(ratios)\n",
    "fitted_model.df_resid = len(ratios) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Scale factor: {0}+/-{1}\".format(np.exp(fitted_model.params[0]),\n",
    "                                       np.exp(fitted_model.params[0]) * fitted_model.bse[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Fitting a distribution is a useful method for finding the scale factor since the significance of outliers is diminished.\n",
    "\n",
    "However, other methods can be used for finding the scaling factor. By fitting a line between the low- and high-resolution points, the slope will be the scale factor. The mean/median of the ratios may also be used. The issue with both of these methods is the presence of outliers. A robust line fitting method, such as [Theil-Sen Regression](https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator), or iteratively rejecting outliers to compute the mean/median of the ratios should be used. \n",
    "\n",
    "All three methods are implemented in `uvcombine.scale_factor.find_scale_factor`. Below is a comparison of the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from uvcombine.scale_factor import find_scale_factor\n",
    "\n",
    "# Distribution\n",
    "distrib_sf = find_scale_factor(lowres_pts[good_pts], highres_pts[good_pts], method='distrib', verbose=True)\n",
    "print(\"Scale factor: {0}+/-{1}\".format(distrib_sf[0], distrib_sf[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Linear fit\n",
    "# The Theil-Sen regression is not memory efficient (it pariwise compares each point). Only use a subset of the points \n",
    "linfit_sf = find_scale_factor(lowres_pts[good_pts][:5000], highres_pts[good_pts][:5000], method='linfit', verbose=True)\n",
    "print(\"Scale factor: {0} 85% CI: {1}-{2}\".format(linfit_sf[0], *linfit_sf[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sigma-clipped statistics\n",
    "sigclip_sf = find_scale_factor(lowres_pts[good_pts], highres_pts[good_pts], method='clippedstats', verbose=True)\n",
    "print(\"Scale factor: Mean {0} Median {1} Std. {2}\".format(sigclip_sf[\"scale_factor_mean\"],\n",
    "                                                          sigclip_sf[\"scale_factor_median\"],\n",
    "                                                          sigclip_sf[\"scale_factor_std\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is significant scatter amongst the three estimates (*note that the Sen-Theil regression only used a sub-set of the points. Too many points will cause a memory error*).\n",
    "\n",
    "The influence of the outliers is significant in the latter two methods, even using the more robust estimators! The issue is two-fold. First, the asymmetry between the low and high points will cause all of the methods to underestimate the scale factor. This is clear from the distribution fit (and is discussed above). Second, the linear fit and sigma-clipped statistics are still heavily influenced by outliers because the outlier population is a moderate fraction of the data. This shows that, so long as there are enough points available, the Cauchy distribution fit should be used to estimate the scale factor."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
